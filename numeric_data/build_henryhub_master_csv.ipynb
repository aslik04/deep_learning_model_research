{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fded832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "Output will be saved to: /Users/adam/Documents/CS310 - 3rd Year Project/numeric_data/henryhub_master.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Base directory (same folder as this notebook)\n",
    "DATA_DIR = Path(\".\")\n",
    "\n",
    "# Input file paths\n",
    "PRICE_FILE      = DATA_DIR / \"henry_hub_01_01_10_to_02_12_25_biz.csv\"\n",
    "STORAGE_FILE    = DATA_DIR / \"storage_daily_ffill.csv\"\n",
    "PRODUCTION_FILE = DATA_DIR / \"us_dry_gas_production_daily_ffill.csv\"\n",
    "USD_FILE        = DATA_DIR / \"usd_index_01_01_10_biz.csv\"\n",
    "WEATHER_FILE    = DATA_DIR / \"weather_biz_dallas_daily.csv\"\n",
    "\n",
    "# Output file path\n",
    "OUTPUT_FILE = DATA_DIR / \"henryhub_master.csv\"\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Output will be saved to: {OUTPUT_FILE.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78276a54",
   "metadata": {},
   "source": [
    "### Step 2 - Helper Function: `load_series`\n",
    "\n",
    "We define a reusable helper function that:\n",
    "- Reads a CSV file\n",
    "- Parses the date column\n",
    "- Sorts by date\n",
    "- Drops duplicate dates (keeps first occurrence)\n",
    "\n",
    "This ensures consistent preprocessing for all input datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717da2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function defined.\n"
     ]
    }
   ],
   "source": [
    "def load_series(path: Path, date_col: str = \"date\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV time-series file with standard preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "        path: Path to CSV file\n",
    "        date_col: Name of the date column (default: 'date')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame sorted by date with duplicates removed\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, parse_dates=[date_col])\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "    \n",
    "    # Drop duplicate dates, keeping the first occurrence\n",
    "    n_before = len(df)\n",
    "    df = df.drop_duplicates(subset=[date_col], keep=\"first\").reset_index(drop=True)\n",
    "    n_dropped = n_before - len(df)\n",
    "    \n",
    "    if n_dropped > 0:\n",
    "        print(f\"  [INFO] Dropped {n_dropped} duplicate date(s) from {path.name}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Helper function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655b031",
   "metadata": {},
   "source": [
    "### Step 3 - Load & Rename Datasets\n",
    "\n",
    "We load each CSV file and standardise column names:\n",
    "- **Price**: `price`\n",
    "- **Storage**: `storage_bcf`\n",
    "- **Production**: `production_bcf`\n",
    "- **USD Index**: `usd_index`\n",
    "- **Weather**: keep existing names (`temp_c`, `temp_max_c`, `temp_min_c`, `hdd`, `cdd` if present)\n",
    "\n",
    "Column name detection handles minor variations (e.g., `storage` vs `storage_bcf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f731928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PRICE data...\n",
      "  Shape: (4146, 2), Date range: 2010-01-04 00:00:00 to 2025-11-24 00:00:00\n",
      "\n",
      "Loading STORAGE data...\n",
      "  Shape: (5804, 2), Date range: 2010-01-01 00:00:00 to 2025-11-21 00:00:00\n",
      "\n",
      "Loading PRODUCTION data...\n",
      "  Shape: (5723, 2), Date range: 2010-01-01 00:00:00 to 2025-09-01 00:00:00\n",
      "\n",
      "Loading USD INDEX data...\n",
      "  Shape: (4156, 2), Date range: 2010-01-01 00:00:00 to 2025-12-05 00:00:00\n",
      "\n",
      "Loading WEATHER data...\n",
      "  Shape: (4156, 4), Weather columns: ['temp_c', 'temp_max_c', 'temp_min_c']\n",
      "  Date range: 2010-01-01 00:00:00 to 2025-12-05 00:00:00\n",
      "\n",
      "âœ… All datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def find_and_rename_column(df: pd.DataFrame, candidates: list, target_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find a column from a list of candidates and rename it to target_name.\n",
    "    Raises an error if no candidate is found.\n",
    "    \"\"\"\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            if col != target_name:\n",
    "                df = df.rename(columns={col: target_name})\n",
    "                print(f\"  [INFO] Renamed '{col}' -> '{target_name}'\")\n",
    "            return df\n",
    "    raise ValueError(f\"Could not find any of {candidates} in columns: {list(df.columns)}\")\n",
    "\n",
    "# ---------- PRICE ----------\n",
    "print(\"Loading PRICE data...\")\n",
    "df_price = load_series(PRICE_FILE)\n",
    "df_price = find_and_rename_column(df_price, [\"price\", \"value\", \"Price\", \"VALUE\"], \"price\")\n",
    "df_price = df_price[[\"date\", \"price\"]]\n",
    "print(f\"  Shape: {df_price.shape}, Date range: {df_price['date'].min()} to {df_price['date'].max()}\")\n",
    "\n",
    "# ---------- STORAGE ----------\n",
    "print(\"\\nLoading STORAGE data...\")\n",
    "df_storage = load_series(STORAGE_FILE)\n",
    "df_storage = find_and_rename_column(df_storage, [\"storage_bcf\", \"storage\", \"Storage\", \"STORAGE\"], \"storage_bcf\")\n",
    "df_storage = df_storage[[\"date\", \"storage_bcf\"]]\n",
    "print(f\"  Shape: {df_storage.shape}, Date range: {df_storage['date'].min()} to {df_storage['date'].max()}\")\n",
    "\n",
    "# ---------- PRODUCTION ----------\n",
    "print(\"\\nLoading PRODUCTION data...\")\n",
    "df_production = load_series(PRODUCTION_FILE)\n",
    "df_production = find_and_rename_column(df_production, [\"production_bcf\", \"production\", \"Production\", \"PRODUCTION\"], \"production_bcf\")\n",
    "df_production = df_production[[\"date\", \"production_bcf\"]]\n",
    "print(f\"  Shape: {df_production.shape}, Date range: {df_production['date'].min()} to {df_production['date'].max()}\")\n",
    "\n",
    "# ---------- USD INDEX ----------\n",
    "print(\"\\nLoading USD INDEX data...\")\n",
    "df_usd = load_series(USD_FILE)\n",
    "df_usd = find_and_rename_column(df_usd, [\"usd_index\", \"usd\", \"USD\", \"dxy\", \"DXY\"], \"usd_index\")\n",
    "df_usd = df_usd[[\"date\", \"usd_index\"]]\n",
    "print(f\"  Shape: {df_usd.shape}, Date range: {df_usd['date'].min()} to {df_usd['date'].max()}\")\n",
    "\n",
    "# ---------- WEATHER ----------\n",
    "print(\"\\nLoading WEATHER data...\")\n",
    "df_weather = load_series(WEATHER_FILE)\n",
    "# Keep all weather columns that exist\n",
    "weather_cols = [\"temp_c\", \"temp_max_c\", \"temp_min_c\", \"hdd\", \"cdd\", \"HDD\", \"CDD\"]\n",
    "available_weather_cols = [col for col in weather_cols if col in df_weather.columns]\n",
    "# Standardise HDD/CDD column names to lowercase\n",
    "rename_map = {\"HDD\": \"hdd\", \"CDD\": \"cdd\"}\n",
    "df_weather = df_weather.rename(columns={k: v for k, v in rename_map.items() if k in df_weather.columns})\n",
    "available_weather_cols = [rename_map.get(c, c) for c in available_weather_cols]\n",
    "available_weather_cols = list(dict.fromkeys(available_weather_cols))  # remove duplicates, preserve order\n",
    "df_weather = df_weather[[\"date\"] + available_weather_cols]\n",
    "print(f\"  Shape: {df_weather.shape}, Weather columns: {available_weather_cols}\")\n",
    "print(f\"  Date range: {df_weather['date'].min()} to {df_weather['date'].max()}\")\n",
    "\n",
    "print(\"\\nâœ… All datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956fdb9",
   "metadata": {},
   "source": [
    "### Step 4 - Merge Datasets\n",
    "\n",
    "We perform a sequential **inner merge** on the `date` column, using the price dataset as the base.\n",
    "This ensures we only keep dates that have data across all sources.\n",
    "\n",
    "After merging, we:\n",
    "- Sort by date\n",
    "- Add an `id` column with constant value `\"HH\"` (Henry Hub identifier for TFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b551eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base (price): 4146 rows\n",
      "After merging storage: 4145 rows\n",
      "After merging production: 4086 rows\n",
      "After merging USD index: 4086 rows\n",
      "After merging weather: 4086 rows\n",
      "\n",
      "âœ… Merge complete. Final shape: (4086, 9)\n",
      "Columns: ['date', 'id', 'price', 'storage_bcf', 'production_bcf', 'usd_index', 'temp_c', 'temp_max_c', 'temp_min_c']\n"
     ]
    }
   ],
   "source": [
    "# Start with price as base\n",
    "df_master = df_price.copy()\n",
    "print(f\"Base (price): {len(df_master)} rows\")\n",
    "\n",
    "# Merge storage\n",
    "df_master = df_master.merge(df_storage, on=\"date\", how=\"inner\")\n",
    "print(f\"After merging storage: {len(df_master)} rows\")\n",
    "\n",
    "# Merge production\n",
    "df_master = df_master.merge(df_production, on=\"date\", how=\"inner\")\n",
    "print(f\"After merging production: {len(df_master)} rows\")\n",
    "\n",
    "# Merge USD index\n",
    "df_master = df_master.merge(df_usd, on=\"date\", how=\"inner\")\n",
    "print(f\"After merging USD index: {len(df_master)} rows\")\n",
    "\n",
    "# Merge weather\n",
    "df_master = df_master.merge(df_weather, on=\"date\", how=\"inner\")\n",
    "print(f\"After merging weather: {len(df_master)} rows\")\n",
    "\n",
    "# Sort by date\n",
    "df_master = df_master.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Add 'id' column for TFT (constant identifier)\n",
    "df_master.insert(1, \"id\", \"HH\")\n",
    "\n",
    "print(f\"\\nâœ… Merge complete. Final shape: {df_master.shape}\")\n",
    "print(f\"Columns: {list(df_master.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeeb868",
   "metadata": {},
   "source": [
    "### Step 5 - Data Quality Checks\n",
    "\n",
    "We perform validation to ensure data integrity:\n",
    "1. Print shape, head, and tail of the dataset\n",
    "2. Assert dates are strictly increasing (no out-of-order rows)\n",
    "3. Assert no duplicate dates exist\n",
    "4. Report NaN counts per column and drop rows with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c733d70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA QUALITY CHECKS\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Shape: 4086 rows Ã— 9 columns\n",
      "\n",
      "ðŸ“‹ First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>storage_bcf</th>\n",
       "      <th>production_bcf</th>\n",
       "      <th>usd_index</th>\n",
       "      <th>temp_c</th>\n",
       "      <th>temp_max_c</th>\n",
       "      <th>temp_min_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>HH</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3117.0</td>\n",
       "      <td>1737233.0</td>\n",
       "      <td>92.3566</td>\n",
       "      <td>-4.2</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>HH</td>\n",
       "      <td>6.19</td>\n",
       "      <td>3117.0</td>\n",
       "      <td>1737233.0</td>\n",
       "      <td>92.2236</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>HH</td>\n",
       "      <td>6.47</td>\n",
       "      <td>3117.0</td>\n",
       "      <td>1737233.0</td>\n",
       "      <td>92.0941</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>HH</td>\n",
       "      <td>7.51</td>\n",
       "      <td>3117.0</td>\n",
       "      <td>1737233.0</td>\n",
       "      <td>92.3684</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>HH</td>\n",
       "      <td>6.56</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>1737233.0</td>\n",
       "      <td>92.1485</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>-12.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  id  price  storage_bcf  production_bcf  usd_index  temp_c  \\\n",
       "0 2010-01-04  HH   6.09       3117.0       1737233.0    92.3566    -4.2   \n",
       "1 2010-01-05  HH   6.19       3117.0       1737233.0    92.2236    -6.4   \n",
       "2 2010-01-06  HH   6.47       3117.0       1737233.0    92.0941    -2.2   \n",
       "3 2010-01-07  HH   7.51       3117.0       1737233.0    92.3684    -4.4   \n",
       "4 2010-01-08  HH   6.56       2850.0       1737233.0    92.1485    -8.3   \n",
       "\n",
       "   temp_max_c  temp_min_c  \n",
       "0        -0.6        -7.8  \n",
       "1        -1.7       -11.1  \n",
       "2         2.8        -7.2  \n",
       "3         0.6        -9.4  \n",
       "4        -4.4       -12.2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Last 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>storage_bcf</th>\n",
       "      <th>production_bcf</th>\n",
       "      <th>usd_index</th>\n",
       "      <th>temp_c</th>\n",
       "      <th>temp_max_c</th>\n",
       "      <th>temp_min_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>2025-08-26</td>\n",
       "      <td>HH</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3217.0</td>\n",
       "      <td>3372895.0</td>\n",
       "      <td>120.9304</td>\n",
       "      <td>21.40</td>\n",
       "      <td>26.1</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>2025-08-27</td>\n",
       "      <td>HH</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3217.0</td>\n",
       "      <td>3372895.0</td>\n",
       "      <td>121.1236</td>\n",
       "      <td>19.20</td>\n",
       "      <td>21.7</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>HH</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3217.0</td>\n",
       "      <td>3372895.0</td>\n",
       "      <td>120.6537</td>\n",
       "      <td>21.65</td>\n",
       "      <td>28.3</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>HH</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3372895.0</td>\n",
       "      <td>120.6028</td>\n",
       "      <td>16.95</td>\n",
       "      <td>18.3</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>HH</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3247480.0</td>\n",
       "      <td>120.6028</td>\n",
       "      <td>25.55</td>\n",
       "      <td>31.1</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  id  price  storage_bcf  production_bcf  usd_index  temp_c  \\\n",
       "4081 2025-08-26  HH   2.82       3217.0       3372895.0   120.9304   21.40   \n",
       "4082 2025-08-27  HH   2.88       3217.0       3372895.0   121.1236   19.20   \n",
       "4083 2025-08-28  HH   2.90       3217.0       3372895.0   120.6537   21.65   \n",
       "4084 2025-08-29  HH   2.88       3272.0       3372895.0   120.6028   16.95   \n",
       "4085 2025-09-01  HH   2.88       3272.0       3247480.0   120.6028   25.55   \n",
       "\n",
       "      temp_max_c  temp_min_c  \n",
       "4081        26.1        16.7  \n",
       "4082        21.7        16.7  \n",
       "4083        28.3        15.0  \n",
       "4084        18.3        15.6  \n",
       "4085        31.1        20.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Check 1: Dates strictly increasing...\n",
      "   âœ… PASSED - All dates are strictly increasing.\n",
      "\n",
      "ðŸ” Check 2: No duplicate dates...\n",
      "   âœ… PASSED - No duplicate dates found.\n",
      "\n",
      "ðŸ” Check 3: NaN counts per column...\n",
      "date              0\n",
      "id                0\n",
      "price             0\n",
      "storage_bcf       0\n",
      "production_bcf    0\n",
      "usd_index         0\n",
      "temp_c            0\n",
      "temp_max_c        0\n",
      "temp_min_c        0\n",
      "   âœ… No NaN values found.\n",
      "\n",
      "============================================================\n",
      "âœ… ALL DATA QUALITY CHECKS PASSED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Shape\n",
    "print(f\"\\nðŸ“Š Shape: {df_master.shape[0]} rows Ã— {df_master.shape[1]} columns\")\n",
    "\n",
    "# Head\n",
    "print(\"\\nðŸ“‹ First 5 rows:\")\n",
    "display(df_master.head())\n",
    "\n",
    "# Tail\n",
    "print(\"\\nðŸ“‹ Last 5 rows:\")\n",
    "display(df_master.tail())\n",
    "\n",
    "# Check 1: Dates strictly increasing\n",
    "print(\"\\nðŸ” Check 1: Dates strictly increasing...\")\n",
    "date_diffs = df_master[\"date\"].diff().dropna()\n",
    "is_strictly_increasing = (date_diffs > pd.Timedelta(0)).all()\n",
    "assert is_strictly_increasing, \"ERROR: Dates are NOT strictly increasing!\"\n",
    "print(\"   âœ… PASSED - All dates are strictly increasing.\")\n",
    "\n",
    "# Check 2: No duplicate dates\n",
    "print(\"\\nðŸ” Check 2: No duplicate dates...\")\n",
    "n_duplicates = df_master[\"date\"].duplicated().sum()\n",
    "assert n_duplicates == 0, f\"ERROR: Found {n_duplicates} duplicate date(s)!\"\n",
    "print(\"   âœ… PASSED - No duplicate dates found.\")\n",
    "\n",
    "# Check 3: NaN counts\n",
    "print(\"\\nðŸ” Check 3: NaN counts per column...\")\n",
    "nan_counts = df_master.isna().sum()\n",
    "total_nans = nan_counts.sum()\n",
    "print(nan_counts.to_string())\n",
    "\n",
    "if total_nans > 0:\n",
    "    print(f\"\\nâš ï¸  Found {total_nans} total NaN value(s). Dropping affected rows...\")\n",
    "    rows_before = len(df_master)\n",
    "    df_master = df_master.dropna().reset_index(drop=True)\n",
    "    rows_after = len(df_master)\n",
    "    print(f\"   Dropped {rows_before - rows_after} row(s). New shape: {df_master.shape}\")\n",
    "else:\n",
    "    print(\"   âœ… No NaN values found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… ALL DATA QUALITY CHECKS PASSED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6aa58",
   "metadata": {},
   "source": [
    "### Step 6 - Save Master CSV\n",
    "\n",
    "We save the merged dataset to `henryhub_master.csv` with columns ordered as:\n",
    "`date`, `id`, `price`, `storage_bcf`, `production_bcf`, `usd_index`, followed by weather columns.\n",
    "\n",
    "The index is not saved to keep the CSV clean for model ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2e2fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Master CSV saved to: /Users/adam/Documents/CS310 - 3rd Year Project/numeric_data/henryhub_master.csv\n",
      "   Final column order: ['date', 'id', 'price', 'storage_bcf', 'production_bcf', 'usd_index', 'temp_c', 'temp_max_c', 'temp_min_c']\n"
     ]
    }
   ],
   "source": [
    "# Define column order\n",
    "core_cols = [\"date\", \"id\", \"price\", \"storage_bcf\", \"production_bcf\", \"usd_index\"]\n",
    "weather_cols_final = [c for c in df_master.columns if c not in core_cols]\n",
    "final_col_order = core_cols + weather_cols_final\n",
    "\n",
    "# Reorder columns\n",
    "df_master = df_master[final_col_order]\n",
    "\n",
    "# Save to CSV\n",
    "df_master.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"âœ… Master CSV saved to: {OUTPUT_FILE.resolve()}\")\n",
    "print(f\"   Final column order: {list(df_master.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e032b8f",
   "metadata": {},
   "source": [
    "### Step 7 - Final Confirmation\n",
    "\n",
    "We verify the saved file and print a summary including:\n",
    "- Output file path\n",
    "- Date range covered\n",
    "- Total row count\n",
    "- File size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfe0748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL CONFIRMATION\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Output file: /Users/adam/Documents/CS310 - 3rd Year Project/numeric_data/henryhub_master.csv\n",
      "ðŸ“… Date range:  2010-01-04 to 2025-09-01\n",
      "ðŸ“Š Total rows:  4,086\n",
      "ðŸ“ Columns:     9 (date, id, price, storage_bcf, production_bcf, usd_index, temp_c, temp_max_c, temp_min_c)\n",
      "ðŸ’¾ File size:   245.21 KB\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ MASTER DATASET BUILD COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify the saved file\n",
    "df_verify = pd.read_csv(OUTPUT_FILE, parse_dates=[\"date\"])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL CONFIRMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ“ Output file: {OUTPUT_FILE.resolve()}\")\n",
    "print(f\"ðŸ“… Date range:  {df_verify['date'].min().strftime('%Y-%m-%d')} to {df_verify['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"ðŸ“Š Total rows:  {len(df_verify):,}\")\n",
    "print(f\"ðŸ“ Columns:     {len(df_verify.columns)} ({', '.join(df_verify.columns)})\")\n",
    "\n",
    "# File size\n",
    "file_size_bytes = OUTPUT_FILE.stat().st_size\n",
    "if file_size_bytes > 1024 * 1024:\n",
    "    file_size_str = f\"{file_size_bytes / (1024 * 1024):.2f} MB\"\n",
    "else:\n",
    "    file_size_str = f\"{file_size_bytes / 1024:.2f} KB\"\n",
    "print(f\"ðŸ’¾ File size:   {file_size_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ MASTER DATASET BUILD COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
